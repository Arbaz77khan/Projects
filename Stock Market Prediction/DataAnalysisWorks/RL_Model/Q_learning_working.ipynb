{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4b8b295-458d-4a45-9624-5199fcac4e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Total Reward: 1.89, Exploration Rate: 0.7403\n",
      "Episode 1, Total Reward: 3.19, Exploration Rate: 0.5480\n",
      "Episode 2, Total Reward: 11.01, Exploration Rate: 0.4057\n",
      "Episode 3, Total Reward: 14.65, Exploration Rate: 0.3003\n",
      "Episode 4, Total Reward: 12.62, Exploration Rate: 0.2223\n",
      "Episode 5, Total Reward: 13.67, Exploration Rate: 0.1646\n",
      "Episode 6, Total Reward: 14.49, Exploration Rate: 0.1218\n",
      "Episode 7, Total Reward: 14.31, Exploration Rate: 0.0902\n",
      "Episode 8, Total Reward: 15.41, Exploration Rate: 0.0668\n",
      "Episode 9, Total Reward: 16.82, Exploration Rate: 0.0494\n",
      "                                           0         1         2\n",
      "-0.059697 -2.917016 -2.520329 2 -1  0.000000  0.162638 -0.000525\n",
      " 0.261311 -2.921657 -2.520329 2 -1  0.031263 -0.004874 -0.001000\n",
      " 0.174668 -2.925137 -2.520329 2 -1  0.013984  0.000902 -0.001000\n",
      " 0.088026 -2.927459 -2.520329 2 -1  0.001699  0.053409 -0.001000\n",
      " 0.001383 -2.933238 -2.520329 2 -1 -0.054200  0.114631  0.000000\n",
      "-0.085260 -2.937886 -1.892661 2 -1 -0.035000  0.000000  0.064721\n",
      " 0.235749 -2.951812 -1.325618 2 -1 -0.019950  0.279117  0.000000\n",
      " 0.149106 -2.969208 -1.143071 2 -1  0.559410  0.000000 -0.001000\n",
      " 0.062464 -2.964529 -2.217337 2 -1 -0.003100  0.000000  0.037650\n",
      "-0.110822 -2.956364 -1.188956 2  1  0.123835 -0.020000  0.000000\n",
      " 0.094734 -2.935492 -0.693095 2  1  0.007023  0.000000 -0.001000\n",
      "-0.078552 -2.911211 -0.693095 2  1  0.031814 -0.005494 -0.000644\n",
      "-0.165194 -2.906571 -0.693095 2  1 -0.009050  0.063773  0.001691\n",
      " 0.069172 -2.903110 -0.693095 2  1  0.061346 -0.015702 -0.000097\n",
      "-0.017471 -2.901951 -0.693095 2  1 -0.004426  0.040912 -0.001000\n",
      "-0.104114 -2.901952 -0.693095 2  1 -0.005000  0.048568 -0.001000\n",
      "-0.190756 -2.904267 -0.693095 2  1  0.074319 -0.015444 -0.001000\n",
      " 0.043610 -2.901954 -0.693095 2  1 -0.010000  0.071175  0.000000\n",
      "-0.043033 -2.904269 -0.693095 2  1 -0.008598  0.033402  0.000634\n",
      "-0.129676 -2.906588 -0.693095 2  1 -0.005000  0.028712 -0.001000\n",
      "-0.216319 -2.908916 -0.693095 2  1  0.003108  0.000000 -0.001000\n",
      "-0.302961 -2.913546 -0.928439 2  1  0.023755  0.000000 -0.001900\n",
      " 0.018047 -2.915860 -0.693095 2  1  0.104205 -0.036285 -0.000998\n",
      "-0.068595 -2.912373 -0.693095 2  1  0.033624 -0.005000  0.000000\n",
      "-0.155238 -2.906570 -0.671845 2  1  0.018370  0.000000  0.000000\n",
      "-0.241881 -2.900775 -0.693095 2  1  0.076741 -0.019000  0.005569\n",
      "-0.328523 -2.892673 -0.693095 2  1 -0.035640  0.130065 -0.001000\n",
      "-0.213262 -2.893839 -0.693095 2  1  0.022502  0.103465  0.000000\n",
      "-0.299905 -2.896164 -0.693095 2  1  0.293418 -0.090774  0.000000\n",
      "-0.386548 -2.886881 -0.086138 2  1  0.046716 -0.005000 -0.001000\n",
      "-0.473190 -2.878737 -0.270406 2  1 -0.017575  0.074076 -0.001000\n",
      "-0.152182 -2.868310  0.205647 2  1 -0.015000  0.075201 -0.000770\n",
      "-0.238824 -2.861401  0.205647 2  1 -0.009247  0.029606 -0.001810\n",
      "-0.325467 -2.867205  0.205647 2  1  0.000000  0.021526  0.004981\n",
      "-0.412110 -2.874152  0.205647 2  1  0.000000  0.068117  0.000000\n",
      "-0.498753 -2.881099  0.205647 2  1  0.022322  0.000475 -0.001000\n",
      "-0.177744 -2.884575  0.205647 2  1 -0.005225  0.110415  0.003275\n",
      "-0.264387 -2.888053 -0.928439 2  1  0.288536 -0.045000  0.000425\n",
      "-0.351029 -2.881078  1.494545 2 -1 -0.012030  0.105464 -0.001000\n",
      "-0.437672 -2.875275  1.494545 2 -1  0.088150  0.000000  0.012880\n",
      "-0.524315 -2.869483  1.797537 2 -1  0.351627 -0.129325  0.009641\n",
      "-0.203306 -2.845150  0.537345 2  1  0.203697  0.004725 -0.001000\n",
      "-0.289949 -2.826566  0.537345 2  1  0.350455 -0.152655  0.000000\n",
      "-0.376591 -2.790614  0.537345 2  1 -0.045600  0.178087  0.000900\n",
      "-0.463234 -2.760525  0.537345 2  1  0.183619 -0.016404 -0.001000\n",
      "-0.549877 -2.743159  0.537345 2  1  0.292051 -0.054389  0.001850\n",
      "-0.228868 -2.722321  0.373183 2  1  0.445386  0.000000  0.043997\n",
      "-0.315511 -2.708375  0.373183 2  1  0.833779  0.000000  0.000000\n",
      "-0.402154 -2.663144  0.373183 2  1 -0.060000  0.397962 -0.001000\n",
      "-0.604249 -2.636500  0.742938 2  1 -0.015000  0.208727  0.000000\n",
      "-0.690892 -2.621485  0.742938 2  1  0.457830 -0.015546  0.044670\n",
      "-0.369883 -2.604166  0.742938 2  1  1.004439  0.000000  0.039612\n",
      "-0.456526 -2.593652  0.742938 2  1  1.609194  0.000000 -0.001000\n",
      "-0.543169 -2.517048  0.373183 2  1 -0.202825  0.786357  0.000000\n",
      "-0.629811 -2.462577  0.742938 2  1  0.519199 -0.088865  0.061120\n",
      "-0.716454 -2.402391  0.742938 2  1  0.817071 -0.130000 -0.001000\n",
      "-0.395446 -2.330624  0.373183 2  1 -0.020000  0.276108  0.000000\n",
      "-0.482088 -2.315449  0.742938 2  1 -0.070000  0.452025  0.000000\n",
      "-0.568731 -2.291194  0.742938 2  1  0.092647 -0.009500  0.000000\n",
      "-0.655374 -2.280875  0.742938 2  1  0.183774  0.000000 -0.001000\n",
      "----------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import random\n",
    "\n",
    "data = pd.read_csv('D:\\Master_Folder\\Data Science Course\\Projects\\StockMarket\\stock_data/SUZLON.NS_2023-01-01_to_2024-11-21_ML_QL.csv')\n",
    "\n",
    "class MarketEnvironment:\n",
    "    def __init__(self, data):\n",
    "        self.data = data.reset_index()\n",
    "        self.current_step = 0\n",
    "        self.done = False\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.done = False\n",
    "\n",
    "        return self.data.iloc[self.current_step][['Temporal_Features', 'Price_Features', 'Upward_Downward_Probability', 'Cluster', 'Anomaly']].values\n",
    "\n",
    "    def step(self, action):\n",
    "\n",
    "        current_close = self.data['Close'].iloc[self.current_step]\n",
    "        next_close = self.data['Close'].iloc[self.current_step + 1] if self.current_step + 1 < len(self.data) else current_close\n",
    "\n",
    "        if action == 0:\n",
    "            reward = next_close - current_close\n",
    "        elif action == 1: \n",
    "            reward = current_close - next_close\n",
    "        elif action == 2:\n",
    "            reward = -0.01\n",
    "\n",
    "        self.current_step +=1\n",
    "        if self.current_step >=len(self.data) - 1:\n",
    "            self.done = True\n",
    "\n",
    "        next_state = (self.data.iloc[self.current_step][['Temporal_Features', 'Price_Features', 'Upward_Downward_Probability', 'Cluster', 'Anomaly']].values if not self.done else None)\n",
    "\n",
    "        return next_state, reward, self.done\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, state_size, action_size, learning_rate=0.1, discount_factor=0.95, exploration_rate=1.0, exploration_decay=0.995): \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.exploration_rate = exploration_rate\n",
    "        self.exploration_decay = exploration_decay\n",
    "        self.q_table = {}\n",
    "\n",
    "    def get_q_values(self, state):\n",
    "        state_tuple = tuple(state)\n",
    "        if state_tuple not in self.q_table:\n",
    "            self.q_table[state_tuple] = np.zeros(self.action_size)\n",
    "        return self.q_table[state_tuple]\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            return np.random.choice(self.action_size)\n",
    "        q_values = self.get_q_values(state)\n",
    "        return np.argmax(q_values)\n",
    "\n",
    "    def learn(self, state, action, reward, next_state): \n",
    "        state_tuple = tuple(state)\n",
    "        next_state_tuple = tuple(next_state) if next_state is not None else None\n",
    "\n",
    "        q_values = self.get_q_values(state)\n",
    "        q_next = np.max(self.get_q_values(next_state)) if next_state_tuple else 0\n",
    "\n",
    "        q_values[action] += self.learning_rate * (reward + self.discount_factor * q_next - q_values[action])\n",
    "\n",
    "        self.exploration_rate *= self.exploration_decay\n",
    "\n",
    "        # print(f\"Exploration Rate after decay: {self.exploration_rate}\")\n",
    "        \n",
    "\n",
    "def train_rl(data, episodes=10):\n",
    "    \n",
    "    env = MarketEnvironment(data)\n",
    "    agent = QLearningAgent(state_size=5, action_size=3)\n",
    "\n",
    "    rewards_per_episode = []\n",
    "\n",
    "    # print(\"pt 1\")\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        # print(\"pt 2\")\n",
    "        while not env.done:\n",
    "            action = agent.choose_action(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            agent.learn(state, action, reward, next_state)\n",
    "\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "        rewards_per_episode.append(total_reward)\n",
    "\n",
    "        # if (episode + 1) % 100 == 0:\n",
    "        print(f\"Episode {episode}, Total Reward: {total_reward:.2f}, Exploration Rate: {agent.exploration_rate:.4f}\")\n",
    "    print((pd.DataFrame(agent.q_table)).T)\n",
    "    print(\"----------------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "    return agent, rewards_per_episode\n",
    "\n",
    "trained_agent, rewards = train_rl(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565e7a69-0c61-42eb-94c6-d4791e4c6b18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
